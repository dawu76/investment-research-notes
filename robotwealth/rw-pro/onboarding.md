### RW Pro onboarding notes

---

Highlights
- RW Pro API key: use API to pull weights into spreadsheet or some other application
- Research pods to house clean data and libraries for cleaning and analyzing data: use Google Colab notebooks to share code snippets of common code patterns / queries
- What's in the "Lab"? Edge database: maintained in Github, in the form of a spreadsheet or Kanban board, that collects info about the edges that RW has investigated before. On main page, click on Lab > Lab HQ > Course Content with videos
- Macro Pod: organized in similar way, with research findings and scripts, and links to any datasets; click on one of the research notebooks to see a walk-through of the findings and code; the notebook will pull code from APIs or from files in Github; occasionally, there may be changes in Colab or libraries that cause the code to break - if that happens, ask for help on Discord.
- There's a RwTools script with common utility snippets for downloading data via API, calculating certain quantities like EWMA, creating a histogram of returns, etc. This script is sourced at the top of many of these notebooks.
- In Colab, go to the settings cog and authorize it to have access to Github; also check the option to access private repos. Also go to Site settings and paste the URL of a Colab notebook into 'Custom snippet notebook URL' field, where the URL is copied from the Lab HQ course > 'Colab: Reproducible Research in the Lab' section. These same instructions are also in that course module. By doing so, you can get easy access to useful research code snippets from any notebook by clicking on '<>' in the LHS menu and filtering in the window. The research code snippets notebook itself is organized by research pod - e.g. equity factors (EF), macro, crypto, etc. When you run the snippets that access the RW data APIs, or run the `rwlab_data_auth()` line, you may get an authorization request in the browser asking you to authorize through your email (?)
- The Risk Premia on Steroids and Trading through Armageddon courses have particularly good lessons.

Brief walk-through of the 'Controlling for non-random stuff' notebook:
- We can very crudely look at a cross-sectional momentum effect, right? We have say if we've got 200 perpetual futures on Binance and we just go, Hey look, a crude measurement of momentum is just how much stuff has moved relative to everything else. And we can proxy momentum like that. We can go look over the last, let's pick a random number last 20 days, how much, what were the returns from every one of the perpetual futures in our universe? Then we can construct some kind of strategy that looks to go long, the stuff that had gone up the most and it looks to go short the stuff that's gone down the most.
- And if, you know, if this is a tradable idea, we would probably expect our crude way of measuring that to show some positive results. But we should also understand that there are other drivers to asset returns, right? We should, we should know for example, that some stuff is more liquid than others. Some stuff is more illiquid than others. Some stuff is more volatile, some stuff is less volatile, some stuff is bigger, some stuff is smaller. And all of these things, you know, volatility, liquidity, size, all those kind of things also have a slightly predictable result or impact on the returns of the assets. So we should be able to improve our simple cross-sectional momentum model by going, Hey look, can I take out the predictable stuff that is caused by differences in volatility, differences in size, differences in liquidity, and things like that. Yeah. So that's what this, this sort of piece of work is, is trying to, trying to teach, right? It's trying to go, Hey look, if we can control for all 52:06 of the non-random stuff that we know, which causes changes in price or returns and things, then that should allow the effect that we're trying to investigate to become more obvious, right? We might even sort of uncover things that we couldn't see before because it was masked by all this sort of other predictable stuff. You know, and we kind of saw this recent, this kind of thing recently when we were looking at VIX and we were looking at calendar effects and days of the week effects and things like that. Remember when we, when we kinda adjusted for all of the strange structure or issues in the VIX forward windows for different months, then being able to adjust for that then allowed us to see some mispricings that we weren't able to see before, right? We could see that, you know, historically December had traded too rich, right? Because the market was over estimating something that it shouldn't have done, right? 'cause it hadn't done that on average. It clearly hadn't done that adjustment correctly. Whereas if we adjust all the sources of non-randomness from our data, then other findings emerge. We kind of saw this recent, this kind of thing recently when we were looking at VIX
- This portion of the notebook looks into crypto 'perpetual swaps'. Here I'm getting the perpetual swap prices and the funding rates. This is my universe of of Binance swaps or Binance perpetual futures. I start in 2020 basically just having Bitcoin, Ethereum, and then we've gradually grown the potential size of the universe to the point that we've got about 200 different instruments that we could possibly trade. If we wanted to construct the biggest cross-sectional momentum basket that we could, we would be trading 200 perpetual swaps right now; we'd be long one, the stuff that went up the most, and we'd be short the the stuff that that went down the most. So the first thing we basically wanna is calculate some kind of momentum feature. Just do the simplest thing, right? You don't have to construct a perfect bit of analysis to start, just start with the dirtiest, quickest thing you can think of. So my dirty thing is just the price, the, the total returns of each perpetual future in our universe over the last 20 days. Create the raw feature, which is the series of total returns -- the asset's price returns plus funding returns over the last 20 days. Then uses those returns to get a measure of cross-sectional momentum, which indicates how much each asset trended up relative to the other assets in the universe. We do this by scaling the returns to get a per-asset Z-score: substracting the mean return and dividing by the standard deviation of returns, both calculated across assets over the trailing window period. This results in a number that's roughly within the [-3, 3] range. This measure gives us a sense of how well each asset is doing in the recent period relative to the others.
- Now I wanna do some real simple analysis. And one of the simplest things is to just look at the correlation of this momentum score with the perpetual future'ss returns in a subsequent period. So we have a dataset containing the momemtum score for each asset / date, and its return in a subsequent period, counting forward from the very next day. We want to find out if higher momentum scores are associated with higher subsequent returns, and vice versa.
