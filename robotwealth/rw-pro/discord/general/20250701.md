### 2025 July Discord notes [up to 2025-07-01]

"Does anyone use a drawdown based trading stop, i.e. when drawdown reaches say 20%, then stop trading until x happens? Or, if a loss of say 5% occurs in a month, then stop trading until month end? I imagine this would be a bad ideas as one misses the bounce backs - but could help in 2008 and 20022. What think?"
- Kris is [probably] gonna tell you there's no edge in your stop loss. Then he'll tell you go check out the Sharpe simulator and see how big a drawdown you can sometimes get. Then he'll say if you know your edge is still valid, embrace the mayhem. And then i think James would say, if you're worried about a 20% drawdown you're trading too big. Reduce your size until you're comfortable trading without the stop. I personally don't use stops...
- Euan: here's the problem with stops: the market doesn't care when you got in or how much you've lost. So you are basing your exit on either your money management or a personal pain point. _**Either of these are better addressed through sizing**._ You should decide to have a position or not based on whether the position has edge or not (I'm ignoring trades done for rebalancing a portfolio. I'm just talking about in/out decisions). **Sometimes edge is correlated with losing money**. If you are trading a trend this will be the case. You are long because you expect the market to go up, but it goes down. Here you exit because you are wrong not because you lost money. **In a mean reverting case, you will often see more edge when you have lost money.** You got in at a Z-score of 2, expecting it to revert to 0. _**If it goes to 3 you shouldn't get out. if anything you should add more.**_ Always, at every moment ask "do i have edge". Yes âž¡ï¸ stay in. This is another very powerful reason to **think in exposures rather than trades**. Entry/exits don't really matter.
- Euan: I do often use stop orders because they are a contingent way to enter positions. "If we go above here I want to buy". But never stop losses.
- Linked tweets by Augustin Lebron / @therobotjames / Scott Phillips: Almost all the time, desire to use stop orders indicates either (a) a cognitive mistake or (b) a trade sizing mistake. I've never used one and don't plan to. James: You're giving away, for free, the option to decide if you actually want to drop that market into the book. And there is high correlation between "market has moved a lot and my stop level got touched" and "wouldn't want to drop a marketable order into this book right now". Sizing smaller and taking more (different) bets probably a more effective approach [than stop losses to avoid big drawdowns]. Scott: The biggest improvement by far I got from my trading systems was when I ditched the boomer-version entry-stop-trail for a continuous forecast.

"Iâ€™m new to trading the [RP on Steroids](https://robotwealth.com/courses/risk-premia-on-steriods/) 7-asset portfolio. Do you first calculate the realized volatility of the portfolio, compare it to your target volatility, and then calculate the target dollar exposure? After that, we should then use the RW API weights to determine how much of each asset to buy right?
- Kris: "You've got it right @George - (target_vol / realised) * `total_portfolio_capital` will give you your nominal allocation. Then, the weights that come out of the API are applied to that nominal allocation. The API gives you equal vol weights, as well as correlation-adjusted equal vol weights. You can use whichever you prefer. I use the latter.

Just wanted to check if Iâ€™m on the right track with this approach. I used the `Simple_ETF_RH` Colab notebook to get the adjusted weights, then calculated daily portfolio returns like this (below)

```
adj_wts %>%
  group_by(date) %>%
  summarise(portfolio_simple_return = sum(fwd_simple_return * adj_weight))
```

After that, I applied `roll_sd` to get the 60 day rolling SD of the portfolio returns. Is this how you would compute the volatility for the 7-asset portfolio?
- Kris: Yep that is a fine approach @George. I prefer an exponentially weighted vol estimate because it smooths things out compared to the rolling window approach (sudden changes when a significant observation drops out of the window). There's a crypto example [here](https://github.com/RWLab/crypto-pod/blob/main/trading/yolo/yolo_simulation.ipynb) that you can adapt to your RPH work if you want [in the 'Simulating YOLO for Estimating Trade Buffer and Sizing' notebook of the [crypto-pod/trading/yolo](https://github.com/RWLab/crypto-pod/tree/main/trading/yolo) directory].
- Euan: This is sort of preemptive, but often people try to get really fancy with vol measurement. It is almost never worth the effort. An EWMA is fine. If you insist on more, don't do it yourself. Sign up for the NYU vol lab [at vlab.stern.nyu.edu].

---

"Any tips/suggestions/book references on simplest method to handle risk/capital allocation to multiple strategies given the following behavior? assuming all strategies have the same sharpe and capacity and correlation between strategies is effectively negligible, whats the simplest way to handle allocation besides equal weight? my issue with that is 85% of the time 1/3 of total capital allocated to strat C would be sitting doing nothing". Strat A: 2 sharpe, X capacity, 100% active, Strat B: 2 sharpe, X capacity, 50% active, Strat C: 2 sharpe, X capacity, 15% active
- not a book or a single method, but a library: I found [pyportfolioopt](https://pyportfolioopt.readthedocs.io/en/latest/UserGuide.html) pretty useful. It supports multiple allocation methods, some of which support constraints. You might get some idea by reading its docs.
- Note: The [cookbook](https://github.com/robertmartin8/PyPortfolioOpt/tree/master/cookbook) directory has notebooks with more examples.

---

A question regarding using volatility scaling for position sizing. If I'm trading an asset, for example TLT, which has an annualized volatility of 10% and I want to have TLT contribute 5% volatility to my portfolio in total then obviously TLT should be half of my portfolio. This is assuming that I hold TLT 365 days a year. However, what if I I'm trying to size a strategy such as window dressing? Let's assume that I hold TLT for half of the month each month. How do I calculate volatility in that case? Is it still the volatility of TLT - in which case I would still use half my portfolio for this abbreviated strategy, or does it in fact change because the" part-time TLT strategy" has a much lower annual volatility given that it's in cash half the time?
- They key point is that volatility doesn't scale linearly with time - it scales with the square root of time (variance scales linearly). So if you have a fractional exposure to an asset, you can **scale by the square root of the proportion of time you have that exposure**. 

Example:
- If you're in the thing 100% of the time: strategy_vol = $\sqrt(1)*asset\\_vol = asset\\_vol$
- If you're in the thing 50% of the time: strategy_vol = $\sqrt(0.5)*asset\\_vol = 0.71 * asset\\_vol$

The TLAQ trading app sizes the end of month trades like this ... You could also do it empirically, i.e. with simulation. To do that, simulate the strategy's exposures and calculate the volatility of returns. That's the strategy vol you use in your sizing calcs. It should come out close to what you get from the formula above - although will deviate a bit due to path dependency.

---

"How do you guys typically handle a factor whose relationship with forward returns is mostly stable (strong correlation), but the sign flips for long periods of time? and i believe the change is a structural non-random change in the relationship. First things that come to mind are:
- identify the root cause of the relationship change, maybe its some condition i didn't account for/model properly and then condition on that
- if i believe its an underlying change in how the market prices the factor, then build some sort of adaptiveness in the signal based on past IC"

Responses
- Kris: "In an ideal world, there'd be a clear causal mechanism that you could incorporate into your model. More likely though, the change will only be obvious in hindsight. One thing you can try is to quantify the stickiness of the sign and magnitude of the IC (scatter plot of today's IC vs tomorrow's IC, autocorrelation plot, etc). If it's positive in one period, is it likely to be positive in the next and vice versa. If the answer to that question is yes, then you could do some sort of rolling IC of the raw factor, and use that flip its sign and maybe even scale it - need to be a bit careful with overfitting though. And depending on just how sticky it is, it might not improve things. But worth looking at."

---

"Does anyone have suggestions about how to deal with survivorship bias if getting the data for companies that didn't survive is tricky? Is it just a bad idea to do analysis without stocks that didn't survive or are there ways to still get at the real effects? I'm specifically looking into the effects of earnings surprises."
- Kris: You could fudge it by limiting your event study to say -1 to +3 days - you're probably not getting many delistings in that window. It won't be perfect, but it might be OK for what you're doing.

"Another question I have is when our portfolioâ€™s realized volatility comes in below our target, do we immediately increase leverage the next day to bring volatility back up to target? And since we use a trade buffer, we are only trading up to the new position * (1 - tb) right?"
- Euan: You wouldn't necessarily increase size. You just make your best guess of future vol and scale to that. Sometimes vol will come in lower and sometimes it will be higher. We just hope it matches on the long run. Wrt trade buffers, what you rebalance to depends on the costs you face. You want to do the thing that gives you the most risk reduction for given cost. _So if costs are per trade, then you size to your perfect point. But if they are proportional to the amount traded then you go to the edge of the band._

---

"I remember that @therobotjames alluded to a way of hedging while making money without getting into details. Can we hedge a portfolio using a selling options strategy with controlled risk?"
- Euan: No. I really think people should get into the habit of **separating edge from hedge**. A hedge is a risk control to reduce your exposure to stuff you don't want. If it makes money it is an edge. And it is really cool if your edges are anti-correlated (they almost never will be) but they still aren't hedges. Splitting stuff like this makes what is going on a lot more clear.
- Kris: Kris A. wrote an excellent [article](https://blog.moontower.ai/if-you-make-money-every-day-youre-not-maximizing/) about this - scroll down about half way, worth a read: "If You Make Money Every Day, Youâ€™re Not Maximizing"

---

"Would it be stupid to spend time looking around for a silly premia between say IBIT and MSTR etc given NAV seems out of whack? What has existed like this before that would be similar and that we have analysed before if anything, would something from Armageddon trading be relevant?"
- Rob Carver wrote a nice piece playing with some premia around MSTR. It had some cool methodology and might be what you are looking for: https://docsend.com/view/fkcwetk5gwk66sz6
- ... Feel like this is more achievable with IBIT or something perhaps.
- ... if it's the vibes coeff and its error margin that gets you paid, you should be doing this on $BMNR and the soon-to-launch Nakamoto and American Bitcoin. with managing slippage, your bias should be skewed to the upside given how good these guys are at collusive grifting.

"If you were estimating weighted average price and you want to use multiple levels insstead of just the first level in the book, what would be a good way to weight the price levels further away from the best? the formula for 1 level looks like: WAP = (bid size x ask price + ask size x bid price) / (bid size + ask size). Trying to think of good way to weight further levels of the book inversely proportional to their distance from best."
- You can exponentially decay it.
- ... or maybe another way would be to use distance away from best measured in price of underlying as opposed to using level number. not sure which would be better. Depends on tick size.

---

Link to Augustin Lebron tweet: "STOP ðŸ‘ TRADING ðŸ‘ OPTIONS! You're making Ken Griffin and Jane Street very very rich! You're making yourself poorer! You have no edge. No amount of education will change that. So go do something useful with your money and your life."
- Euan: true, not necessarily, not necessarily, false, like that's just your opinion bro.
- Maybe he is referring to alpha more than beta?  A lot of the things we, as retail traders, can do with options are smart beta/risk premias. I suppose actually finding alpha in options is much harder than that and would need also complex execution infrastructure and book management.
- Euan: agustin is generally on the side of non-bullshit but he doesn't actually trade options, and i don't think he has since he left the street. so he isn't really up on the current state of the market. and, probably more importantly, what he says is true for 90% of retail
- Euan: The edge is very thin. Jane Street etc big advantage is the bid ask spread. Thats the big alpha.

---

If it's of any help, I had that same "Non-interactive session" issue using my own Jupyter instance, and after much back-and-forth ended up with this code: https://pastebin.com/aYN5vfgX
- tl;dr; define GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_EMAIL, run rw_auth() to get the login link, copy-paste "token" from the return url and run rw_auth_return() on it.
- unfortunately I don't remember how I created the client id/secret, but probably did something at https://console.cloud.google.com/

```
# =============================================================
# RW Auth
#
# NB: Standard rw function to auth with lab/gcs does not work
# because Jupyter is not in "interactive" mode, which causes
# all kinds of problems when they use gargle lib to do auth.
#
# Therefore we rolled our own gcs auth, also using our own
# client id (else we'd need tidyverse's client secret).
#
# Usage:
#
# rw_auth() -> Gives the link we need to open in browser to log in
# rw_auth_return(return_url) -> Token exchange and authorize google storage
#
# We try to cache token on disk, so we only need to log in when actually needed.
# =============================================================
 
GOOGLE_CLIENT_ID <- 'xxx'
GOOGLE_CLIENT_SECRET <- 'yyy'
GOOGLE_EMAIL <- 'your@gmail.com'
 
get_token_cache_path <- function() {
  cache_dir <- '/home/jovyan/disposable/cache/rw_google_auth'
  if (!dir.exists(cache_dir)) {
    dir.create(cache_dir, recursive = TRUE)
  }
  file.path(cache_dir, "google_token.rds")
}
 
is_token_valid <- function(token) {
  if (is.null(token) || is.null(token$credentials)) {
    return(FALSE)
  }
 
  # Check if token has expiry info
  if (!is.null(token$credentials$expires_in) && !is.null(token$credentials$.expires)) {
    # Token has expiry info, check if it's still valid (with 5 minute buffer)
    expires_at <- token$credentials$.expires
    return(Sys.time() < (expires_at - 300))  # 5 minute buffer
  }
 
  # If no expiry info, try to use the token and see if it works
  # This is a simple check - you might want to make an actual API call
  return(TRUE)
}
 
extract_code_from_url <- function(url) {
  # Extract the query string part
  query <- sub(".*\\?", "", url)
 
  # Split into key-value pairs
  parts <- strsplit(query, "&")[[1]]
  kv <- strsplit(parts, "=")
 
  # Build named list
  params <- setNames(
    vapply(kv, function(x) URLdecode(x[2]), character(1)),
    vapply(kv, function(x) x[1], character(1))
  )
 
  # Return the "code" parameter (or NULL if missing)
  params[["code"]]
}
 
refresh_token_if_needed <- function(token) {
  if (is.null(token) || is.null(token$credentials)) {
    return(NULL)
  }
 
  # Check if we have a refresh token
  if (is.null(token$credentials$refresh_token)) {
    cat("No refresh token available. Need to re-authenticate.\n")
    return(NULL)
  }
 
  cat("Refreshing access token...\n")
 
  tryCatch({
    library(httr)
 
    # Make refresh request
    refresh_response <- POST(
      "https://oauth2.googleapis.com/token",
      body = list(
        client_id = GOOGLE_CLIENT_ID,
        client_secret = GOOGLE_CLIENT_SECRET,
        refresh_token = token$credentials$refresh_token,
        grant_type = "refresh_token"
      ),
      encode = "form"
    )
 
    if (status_code(refresh_response) == 200) {
      new_credentials <- content(refresh_response)
 
      # Update the token credentials
      token$credentials$access_token <- new_credentials$access_token
      token$credentials$expires_in <- new_credentials$expires_in
      token$credentials$.expires <- Sys.time() + as.numeric(new_credentials$expires_in)
 
      # Keep the original refresh token if not provided in response
      if (is.null(new_credentials$refresh_token)) {
        new_credentials$refresh_token <- token$credentials$refresh_token
      } else {
        token$credentials$refresh_token <- new_credentials$refresh_token
      }
 
      cat("Token refreshed successfully!\n")
      return(token)
    } else {
      cat("Failed to refresh token. Status:", status_code(refresh_response), "\n")
      return(NULL)
    }
  }, error = function(e) {
    cat("Error refreshing token:", e$message, "\n")
    return(NULL)
  })
}
 
# Load cached token if available and valid
load_cached_token <- function() {
  cache_path <- get_token_cache_path()
 
  if (!file.exists(cache_path)) {
    cat("No cached token found.\n")
    return(NULL)
  }
 
  tryCatch({
    token <- readRDS(cache_path)
    cat("Loaded cached token.\n")
 
    # Check if token is still valid
    if (is_token_valid(token)) {
      cat("Cached token is still valid.\n")
      return(token)
    } else {
      cat("Cached token has expired, attempting to refresh...\n")
      refreshed_token <- refresh_token_if_needed(token)
 
      if (!is.null(refreshed_token)) {
        # Save the refreshed token
        saveRDS(refreshed_token, cache_path)
        cat("Token refreshed and saved to cache.\n")
        return(refreshed_token)
      } else {
        cat("Could not refresh token. Need to re-authenticate.\n")
        return(NULL)
      }
    }
  }, error = function(e) {
    cat("Error loading cached token:", e$message, "\n")
    return(NULL)
  })
}
 
# Save token to cache
save_token_to_cache <- function(token) {
  cache_path <- get_token_cache_path()
  tryCatch({
    saveRDS(token, cache_path)
    cat("Token saved to cache at:", cache_path, "\n")
  }, error = function(e) {
    cat("Error saving token to cache:", e$message, "\n")
  })
}
 
rw_auth <- function(force_reauth = FALSE) {
  # Try to load cached token first (unless forced to re-authenticate)
  if (!force_reauth) {
    cached_token <- load_cached_token()
    if (!is.null(cached_token)) {
      cat("Using cached authentication token.\n")
      cat("Authenticating GoogleCloudStorage with cached token...\n")
      googleCloudStorageR::gcs_auth(token = cached_token, email = "stian@hahabiz.no")
      cat("Authentication complete using cached token!\n")
      return(invisible(cached_token))
    }
  }
 
  # If no valid cached token, proceed with manual auth flow
  url <- paste0(
    'https://accounts.google.com/o/oauth2/v2/auth?client_id=',
    GOOGLE_CLIENT_ID,
    '&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email',
    '&redirect_uri=',
    'http://localhost',
    '&response_type=code&state=4bc6806258819a59c5a515d8e6f67b73&access_type=offline&prompt=consent'
  )
  cat("No valid cached token found. Starting manual authentication...\n")
  cat("Visit this URL:\n")
  cat(url, "\n\n")
  cat("When done, copy-paste the return-url you land on, and run rw_auth_complete(<return_url>)\n")
}
 
rw_auth_return <- function(url) {
 
  # Extract code from return url
  code <- extract_code_from_url(url)
  cat("Extracted code:", code, "\n")
 
  # Exchange code for token
  library(httr)
  redirect_uri <- "http://localhost"
  app <- oauth_app("google", key = GOOGLE_CLIENT_ID, secret = GOOGLE_CLIENT_SECRET)
  endpoint <- oauth_endpoints("google")
  token <- oauth2.0_access_token(
    endpoint = endpoint,
    app = app,
    code = code,
    redirect_uri = redirect_uri
  )
 
  # Add expiry time if expires_in is present
  if (!is.null(token$expires_in)) {
    token$.expires <- Sys.time() + as.numeric(token$expires_in)
  }
 
  # Make proper Token2 object
  oauth_app_obj <- oauth_app("google", key = GOOGLE_CLIENT_ID, secret = GOOGLE_CLIENT_SECRET)
  token2 <- Token2.0$new(
    app = oauth_app_obj,
    endpoint = oauth_endpoints("google"),
    params = list(as_header = TRUE),
    credentials = token
  )
  class(token2) <- c("gargle_token", class(token2))  # optionally tag for gargle compatibility
 
  # Save token to cache
  save_token_to_cache(token2)
 
  cat("Authenticating GoogleCloudStorage...\n")
  googleCloudStorageR::gcs_auth(token = token2, email = "stian@hahabiz.no")
  cat("All done! Token cached for future use.\n")
 
  return(invisible(token2))
}
 
# Convenience function to clear cached token
clear_auth_cache <- function() {
  cache_path <- get_token_cache_path()
  if (file.exists(cache_path)) {
    file.remove(cache_path)
    cat("Authentication cache cleared.\n")
  } else {
    cat("No authentication cache found.\n")
  }
}
```
