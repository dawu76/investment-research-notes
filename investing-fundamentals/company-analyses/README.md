### Company analyses

---

#### $GOOG

[TCI fund letter](https://www.tcifund.com/files/corporateengageement/alphabet/15th%20November%202022.pdf) to $GOOG board
- ...

[Semi-analysis Substack](https://www.semianalysis.com/p/tpuv5e-the-new-benchmark-in-cost): "TPUv5e: The New Benchmark in Cost-Efficient Inference and Training for <200B Parameter Models"
- "During its Cloud Next 2023 event, Google has announced general availability of its latest AI chip, the TPUv5e (TPUv5 lite), and it is a game changer ... TPUv5e also enables Google to inference models that are larger than OpenAI at the same cost as OpenAI’s smaller model. This will massively help Google level the playing field, because they can play the brute force game that no one else can. OpenAI will have to rely on being much smarter with their chips and algorithms due to the massive compute deficit versus Google. AI chips from Amazon (Trainium/Inferentia), Meta (MTIA), and Microsoft (Athena) are all nowhere close to where Google is."
- "The TPUv5 and the smaller sibling, TPUv5e, are clearly not designed for peak performance at the cost of everything else. They are both significantly lower power, memory bandwidth, and FLOPS than Nvidia’s H100. This is a conscious decision by Google, and not just an indicator of worse chip design." Because Google designs and acquires their own chips through Broadcom, their TCO for these chips is dominated by OpEx, not CapEx. For customers who purchase chips from Nvidia, who earns massive GM$ from them, the TCO is dominated by Capex, so it makes sense for the H100 focus more on performance rather than on efficiency.

[Semi-analysis Substack](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini): Google Gemini Eats The World – Gemini Smashes GPT-4 By 5X, The GPU-Poors
- "The sleeping giant, Google has woken up, and they are iterating on a pace that will smash GPT-4 total pre-training FLOPS by 5x before the end of the year. The path is clear to 20x by the end of next year given their current infrastructure buildout. Whether Google has the stomach to put these models out publicly without neutering their creativity or their existing business model is a different discussion."
- On the GPU-Poor: "there are a whole host of startups and open-source researchers who are struggling with far fewer GPUs. They are spending significant time and effort attempting to do things that simply don’t help, or frankly, matter ... spending countless hours agonizing on fine-tuning models with GPUs that don’t have enough VRAM ... anextremely counter-productive use of their skills and time ...  Yes, being efficient with GPUs is very important, but in many ways, that’s being ignored by the GPU-poors. They aren’t concerned with efficiency at scale, and their time isn’t being spent productively. What can be done commercially in their GPU-poor environment is mostly irrelevant to a world that will be flooded by more than 3.5 million H100s by the end of next year."
- On what the GPU-Poors should be focused on: "The underdogs should be focusing on tradeoffs that improve model performance or token to token latency by upping compute and memory capacity requirements [while] reducing memory bandwidth because that’s what the edge needs. They should be focused on efficient serving of multiple finetuned models on shared infrastructure without paying the horrendous cost penalties of small batch sizes ... [not] on memory capacity constraints or quantizing too far while covering their eyes about real quality decreases ... Hopefully the open efforts are redirected towards evaluations, speculative decoding, MoE, open IFT [instruction fine-tuened] data, and clean pre-training datasets with over 10 trillion tokens."
- The GPU-Poor includes some big names but they will find it hard to beat Nvidia: "Some of the most well recognized AI firms, HuggingFace, Databricks (MosaicML), and Together are also part of this GPU-poor group. In fact, they may be the most GPU-poor groups out there with regard to both the number of world class researchers per GPU and the number of GPUs versus the ambition/potential customer demand. They have world class researchers, but all of them are limited by working on systems with orders of magnitude less capabilities ... The picks and shovels training and inference ops firms (Databricks, HuggingFace, and Together) are behind their chief competition, who also happens to also be the source of almost all of their compute ... These firms have tremendous inbound from enterprises on training real models, and on the order of thousands of H100s coming in, but that won’t be enough to grab much of the market ... Nvidia is eating their lunch with multiple times as many GPUs in their DGX Cloud service and various in-house supercomputers. Nvidia’s DGX Cloud offers pretrained models, frameworks for data processing, vector databases and personalization, optimized inference engines, APIs, and support from NVIDIA experts to help enterprises tune models for their custom use cases."
- On dense vs. sparse, mixture-of-expert (MoE) models: "The GPU poor are still mostly using dense models because that’s what Meta graciously dropped on their lap with the LLAMA series of models. Without God’s Zuck’s good grace, most open source projects would be even worse off. If they were actually concerned with efficiency, especially on the client side, they’d be running sparse model architectures like MoE, training on these larger datasets, and implementing speculative decoding like the Frontier LLM Labs (OpenAI, Anthropic, Google Deepmind)."
- On how public leaderboards and evaluation benchmarks are broken: "These startups and open-source researchers are using larger LLMs to fine-tune smaller models for leaderboard style benchmarks with broken evaluation methods that give more emphasis to style rather than accuracy or usefulness ... generally ignorant that pretraining datasets and IFT data need to be significantly larger/higher quality for smaller open models to improve in real workloads. ... while there is a lot of effort in the closed world to improve this, the land of open benchmarks is pointless and measures almost nothing useful. For some reason there is an unhealthy obsession over the leaderboard-ification of LLMs, and meming with silly names for useless models (WizardVicunaUncensoredXPlusPlatypus) ... HuggingFace’s leaderboards show how truly blind they are because they actively hurting the open source movement by tricking it into creating a bunch of models that are useless for real usage."
- The only other firm who can compete with Nvida in terms of sheer computing power: Google. Look at total advanced chips added by quarter: "Here we give OpenAI every benefit of the doubt - that the number of total GPUs they have will 4x over 2 years. For Google, [if] we ignore their entire existing fleet of TPUv4 (Pufferfish), TPUv4 lite, and internally used GPUs [as well as their] TPUv5e (lite), despite that likely being the workhorse for inference of smaller language models, [and limit] Google’s growth in this chart [to] only TPUv5 (Viperfish)." Given all this, the number of advanced chips added per quarter by $GOOG will dwarve that of any other company.

---

#### $META

[Fallacy Alarm Substack](https://fallacyalarm.substack.com/p/meta-3q22-digest-and-revisiting-my): Meta 3Q22 Digest & revisiting my original investment case
- ...

[Investment Ideas by Antonio Substack](https://antoniolinares.substack.com/p/meta-investing-for-an-exponential): $META - Investing for an exponential future
- ...

[Gupta Media post](https://www.guptamedia.com/insights/meta-conversion-breakdowns-return): Meta's conversion breakdowns return
- ...

[Rihard Jarc blog](https://uncoveralpha.substack.com/p/meta-short-term-profits-vs-long-term): $META - short-term profits vs. long-term vision
- ...

[Commonstock post](https://commonstock.com/post/fbd93762-8ebc-461a-925f-9ab41ecb1eac): WhatsApp: Meta's Next Growth Engine
- ...

[Seeking Alpha: Cavanaugh Research](https://archive.is/41yF0): Meta's Q4 Earnings Preview: What You Need To Know
- Assuming the average analyst consensus estimate as a benchmark, it is suggested that Meta's Q4 sales may decrease by around 6.1% compared to the same quarter in 2021. Additionally, analysts have provided EPS estimates ranging from $1.41 to $2.66, with an average of $2.21, which would indicate a year-over-year decline of almost 40%!
- One potential source for upside could be anchored on a recovery in digital advertising. Investors should consider that macro conditions, or at least sentiment and expectations, have materially improved in Q4 2022 as compared to Q3 2022 - with leading banks such as JPMorgan Chase (JPM) now expecting a softish landing. Notably, going into Q2 and Q3 reporting, some advertising players, notably Snap Inc. (SNAP) warned investors about the deteriorating advertising environment. But we have not heard any such warning from any player going into Q4. Investors should also not fail to disregard implications from the China reopening on advertising - as a bullish macro driver for the global economy.
- Martin Sorrell's comments: "I think that is the big thing here [the reopening of China]. And remember... Alphabet/ Google, Amazon, Meta, there second biggest profit centers have historically been outbound Chinese -- Chinese companies targeting business abroad."
- According to Meta's projections, the negative impact of foreign currency on year-over-year total revenue growth in the fourth quarter was estimated to be around 7%, based on current exchange rates at the time of guidance. Since then, however, the U.S. Dollar Index has depreciated materially - by almost 8%!

[fb.com blog](https://about.fb.com/news/2023/03/facebook-today-and-tomorrow/): FB today and tomorrow
- Big themes
  - Major milestone on Facebook: 2 billion daily actives — the highest it’s ever been.
  - Our focus this year: artificial intelligence, messaging, creators and monetization.
- AI-Powered Discovery
  - We’re using AI to recommend all types of content beyond Reels: photos, text, groups, short- and long-form videos and more. This is a massive technological undertaking. AI-powered recommendations are delivered and refined in response to people’s direct feedback through tools like our Show More or Show Less feature
  - Using AI to find public Group content based on your interests: we can now show you highly relevant content from public Groups in your Feed without you having to do any searching or depending on word of mouth to uncover a Group.
  - We use AI to identify creators who may appeal to your interests and to improve how we recommend Reels and other great content from creators in your Feed. More than 140 billion Reels are played across Facebook and Instagram each day.
- Creators and Monetization
  - We’re heavily invested in building tools for creators. We’ve been working to simplify the Facebook experience for creators, rolling out more formats for creative expression, providing tools to help grow and manage fan communities, and expanding ways to earn income as a creator on Facebook.
  - We're focused on adapting and enhancing our monetization tools for short-form video. We’ll continue expanding our ads on Facebook Reels tests to help more creators earn ad revenue for their Reels and grow virtual gifting via Stars on Reels.
- Messaging
  - Over 140 billion messages are sent across our apps every day. On Instagram, people already reshare Reels nearly 1 billion times daily through DMs and on Facebook we see private sharing of Reels growing strongly as well.
  - We started introducing community chats to some Facebook Groups last year as a way for people to connect more deeply with their online communities in real time. Across Facebook and Messenger, we saw the number of people trying community chats increase by 50% in December 2022. 

---

#### Anti-obesity drugs coming to market

[NYTimes](https://www.nytimes.com/2023/08/17/health/weight-loss-drugs-obesity-ozempic-wegovy.html): "We Know Where New Weight Loss Drugs Came From, but Not Why They Work" [2023-08]
- companies: Novo Nordisk, Eli Lilly, Amgen
- "At Novo Nordisk, chemists began by using a well-known trick. They loosely attached GLP-1 to a blood protein that kept it stable enough to remain in circulation for at least 24 hours. But when GLP-1 slips off the protein, enzymes in the blood quickly degrade it. So chemists had to alter the hormone’s building blocks — a chain of amino acids — to find a more durable variant. After tedious trial and error, Novo Nordisk produced liraglutide, a GLP-1 drug that lasted long enough for daily injections. They named it Victoza, and the F.D.A. approved it as a treatment for diabetes in 2010. It had an unexpected side effect: slight weight loss."
- Novo Nordisk, which today has 45.7 percent of the global insulin market, thought of itself as a diabetes company. Period. But one company scientist, Lotte Bjerre Knudsen, could not stop thinking about tantalizing results from studies with liraglutide, the GLP-1 drug that lasted long enough to be injected just once a day ... Other studies by academic researchers found that rats lost their appetites if GLP-1 was injected into their brains. Human subjects who got an intravenous drip of GLP-1 ate 12 percent less at a lunch buffet than those who got a placebo."
- On fighting the view that obesity is entirely due to lack of willpower: "Dr. Tschöp’s former colleague at Lilly, Dr. Richard Di Marchi, who also was an executive at Novo Nordisk: “There was very little interest in the industry in doing this,” said Dr. Di Marchi, now at Indiana University. “Obesity was not thought to be a disease. It was looked at as a behavioral problem ... "So why not study liraglutide as both a diabetes drug and an obesity drug, Dr. Knudsen asked. She faced resistance in part because some company executives were convinced that obesity resulted from a lack of willpower." One of the champions of investigating GLP-1 for weight loss, Mads Krogsgaard Thomsen, the current chief executive of the Novo Nordisk Foundation and former chief scientific officer of the company, said he “had to spend half a year convincing my C.E.O. that obesity is not just a lifestyle condition.”
- "Despite the progress on weight loss, Novo Nordisk continued to focus on diabetes, trying to find ways to make a longer-lasting GLP-1 so patients would not have to inject themselves every day. The result was a different GLP-1 drug, semaglutide, that lasted long enough that patients had to inject themselves only once a week. It was approved in 2017 and is now marketed as Ozempic. It also caused weight loss — 15 percent, which is three times the loss with Saxenda, the once-a-day drug, although there was no obvious reason for that. Suddenly, the company had what looked like a revolutionary treatment for obesity.
- "In 2021, Novo Nordisk got approval from the F.D.A. to market the same drug for obesity with a weekly injection at a higher maximum dose. It named the drug Wegovy... Wegovy is in such demand, though, that the company is unable to make enough, its spokeswoman Ambre James-Brown said. So for now, while it ramps up production, the company sells the drug only in Norway, Denmark, Germany and the United States. And at pharmacies in those countries, shortages are frequent."
- The drugs, said Randy Seeley, an obesity researcher at the University of Michigan, are not correcting for a lack of GLP-1 in the body — people with obesity make plenty of GLP-1. Instead, the drugs are exposing the brain to hormone levels never seen in nature. Patients taking Wegovy are getting five times the amount of GLP-1 that they would produce in response to a Thanksgiving dinner, Dr. Seeley said. And in the brain, “the drugs go to unusual places.” They are not just going to areas thought to control overeating... GLP-1, because of its chemical structure, should not even get into some areas of the brain where it slips in. “Nobody understands that,” Dr. Seeley said.
- Lilly’s diabetes drug, tirzepatide or Mounjaro, is expected to get F.D.A. approval for obesity this year. It hooks GLP-1 to another gut hormone, GIP. GIP, on its own, produces, at best, a modest weight loss. But the two-hormone combination can allow people to lose a median of about 20 percent of their weight ... Lilly has another drug, retatrutide, that, while still in early stages of testing, seems to elicit a median 24 percent weight loss. Amgen’s experimental drug, AMG 133, could be even better, but is even more of a puzzle. It hooks GLP-1 to a molecule that blocks GIP. There is no logical explanation for why seemingly opposite approaches would work.
